{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17c33465",
   "metadata": {},
   "source": [
    "# Here are the learnings from LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eceb79b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93b5126",
   "metadata": {},
   "source": [
    "[SWE-Best Practise]\n",
    "\n",
    "dotenv is used when we have a `.env` file in the source repository. It is used to hide secrets, passwords and API tokens instead of coding it directly into python scripts, which you would naturally hard-code in. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de64a96",
   "metadata": {},
   "source": [
    "# Module 1: Agent Foundations\n",
    "\n",
    "Here are a list of foundation tools with Langchain.\n",
    "\n",
    "Requirements:\n",
    "- `.env` file with API tokens to LLM model and Langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5e8665",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# init_chat_model initiates the LLM model you will use\n",
    "model = init_chat_model(model=\"gpt-5-nano\")\n",
    "\n",
    "# invoke triggers the LLM with your message prompt\n",
    "response = model.invoke(\"What's inside the day?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a7663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs the LLM's response\n",
    "print(response.content)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# pretify output for the metadata including model name and token usage\n",
    "pprint(response.response_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e11fad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models can be customised with the following parameters\n",
    "model = init_chat_model(\n",
    "    model=\"gpt-5-nano\",\n",
    "    # Kwargs passed to the model:\n",
    "    temperature=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e20b57",
   "metadata": {},
   "source": [
    "The following parameters allows customisable LLM responses:\n",
    "- temperature = control's the model's creativity\n",
    "    - 0 - 0.2 = deterministic - used for math\n",
    "    - 0.3 - 0.6 = focused - used for general QA\n",
    "    - 0.7 - 1.0 = creative - used for creative writing\n",
    "\n",
    "- max_tokens = how long the model's response will be\n",
    "    - 0 - 50 = short answers\n",
    "    - 51 - 500 = short explanations\n",
    "    - 1000+ = long explanations\n",
    "\n",
    "- timeout = how long the client waits before aborting request\n",
    "    - 0 - 30 = Fast API calls\n",
    "    - 30 - 60 = Long outputs\n",
    "\n",
    "- max_retries = how many times to re attempt before cancelling the request\n",
    "    - 0 - 2 = minimal\n",
    "    - 3 - 5 = production safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d33420e",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_chat_model(model=\"claude-sonnet-4-5\")\n",
    "ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3b8fa1",
   "metadata": {},
   "source": [
    "LangChain benefits from being model agnostic, they host the models so we can experiment with different ones all the time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04f3bff",
   "metadata": {},
   "source": [
    "# Agent Foundations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8cd94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "# creates an agent focused on chat behaviour\n",
    "agent = create_agent(model=\"gpt-5-nano\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f0e315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import AIMessage, HumanMessage\n",
    "\n",
    "response = agent.invoke(\n",
    "    # Human prompt is first here\n",
    "    {\"messages\": [HumanMessage(content=\"What's the capital of the Moon?\"),\n",
    "    AIMessage(content=\"The capital of the Moon is Luna City.\"),\n",
    "    HumanMessage(content=\"Interesting, tell me more about Luna City\")]}\n",
    ")\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aedcc8c",
   "metadata": {},
   "source": [
    "\"messages\" is a dictionary where Human and AI messages are stored for contexual learning and understanding for the AI. It can understand what's been said before in order to not repeat itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd50639",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token, metadata in agent.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"Tell me all about Luna City, the capital of the Moon\")]},\n",
    "    stream_mode=\"messages\"\n",
    "):\n",
    "\n",
    "    # token is a message chunk with token content\n",
    "    # metadata contains which node produced the token\n",
    "    \n",
    "    if token.content:  # Check if there's actual content\n",
    "        print(token.content, end=\"\", flush=True)  # Print token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7968a02",
   "metadata": {},
   "source": [
    "\"stream\" is used to stream the AI message back to the human so the percieved time is shorter than it actually is. AI message response is measured in number of seconds due to the time it takes rather than miliseconds in normal API calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f77ca65",
   "metadata": {},
   "source": [
    "## Agent Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f34d7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a science fiction writer, create a capital city at the users request.\"\n",
    "\n",
    "# agent with the configurations\n",
    "scifi_agent = create_agent(\n",
    "    model=\"gpt-5-nano\",\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "\n",
    "# question invoked by human\n",
    "question = HumanMessage(content=\"What's the capital of the moon?\")\n",
    "response = scifi_agent.invoke( # scifi agent with the system prompt\n",
    "    {\"messages\": [question]}\n",
    ")\n",
    "\n",
    "print(response['messages'][1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5257bfeb",
   "metadata": {},
   "source": [
    "`system_prompt` allows the model to be configured to how you want the model to behave. This is a form of prompt engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041d4b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic prompting\n",
    "system_prompt = \"You are a science fiction writer, create a capital city at the users request.\"\n",
    "\n",
    "# example based prompting\n",
    "system_prompt = \"\"\"\n",
    "\n",
    "You are a science fiction writer, create a space capital city at the users request.\n",
    "\n",
    "User: What is the capital of mars?\n",
    "Scifi Writer: Marsialis\n",
    "\n",
    "User: What is the capital of Venus?\n",
    "Scifi Writer: Venusovia\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# structured prompting\n",
    "system_prompt = \"\"\"\n",
    "\n",
    "You are a science fiction writer, create a space capital city at the users request.\n",
    "\n",
    "Please keep to the below structure.\n",
    "\n",
    "Name: The name of the capital city\n",
    "\n",
    "Location: Where it is based\n",
    "\n",
    "Vibe: 2-3 words to describe its vibe\n",
    "\n",
    "Economy: Main industries\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62287b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structured prompting with OOP classes\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# remember a class is like a Microsoft Forms, fixed categories that all users must follow\n",
    "class CapitalInfo(BaseModel):\n",
    "    name: str\n",
    "    location: str\n",
    "    vibe: str\n",
    "    economy: str\n",
    "\n",
    "agent = create_agent(\n",
    "    model='gpt-5-nano',\n",
    "    system_prompt=\"You are a science fiction writer, create a capital city at the users request.\",\n",
    "    response_format=CapitalInfo\n",
    ")\n",
    "question = HumanMessage(content=\"What is the capital of The Moon?\")\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [question]}\n",
    ")\n",
    "\n",
    "response[\"structured_response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d4254d",
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_info = response[\"structured_response\"]\n",
    "\n",
    "capital_name = capital_info.name\n",
    "capital_location = capital_info.location\n",
    "\n",
    "print(f\"{capital_name} is a city located at {capital_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a1e6ea",
   "metadata": {},
   "source": [
    "Depending on the use case you can use OOP classes to allow the LLM to associate with the class attributes like name or location. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73868946",
   "metadata": {},
   "source": [
    "# Agent Tools\n",
    "Tools allows agents to access data, execute tasks and call other agents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4142fc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def square_root(x: float) -> float:\n",
    "    \"\"\"Calculate the square root of a number\"\"\"\n",
    "    return x ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03af4e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the name and description can be override if you prefer\n",
    "@tool(\"square_root\", description=\"Calculate the square root of a number\")\n",
    "def tool1(x: float) -> float:\n",
    "    return x ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf1db7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions can be invoked with the parameter and the value\n",
    "tool1.invoke({\"x\": 467})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62019f91",
   "metadata": {},
   "source": [
    "`@tool` decorator turns the function into a tool the agent can use. The function name and docstring needs to be detailed for the LLM to understand what the function does.\n",
    "\n",
    "LLM's will call these functions with the invoke method. Invoke takes the parameters and and value for that parameter\n",
    "\n",
    "Remember, a decorator sits above a function and can wrap a function into certain behaviours or classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "684f27eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "√467 ≈ 21.61018278497431\n",
      "\n",
      "Rounded: ≈ 21.6102 (4 decimal places).\n",
      "[HumanMessage(content='What is the square root of 467?', additional_kwargs={}, response_metadata={}, id='c0dfb252-944f-4dc0-8310-ec10d9b69426'),\n",
      " AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1240, 'prompt_tokens': 158, 'total_tokens': 1398, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1216, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CpiFaLmDhIhnpPJYxa7Ztxjn5U295', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019b4814-837e-72a2-91f7-c0e3fc38a19d-0', tool_calls=[{'name': 'square_root', 'args': {'x': 467}, 'id': 'call_msX6uPgYhCn9O4LmuAPfCZx6', 'type': 'tool_call'}], usage_metadata={'input_tokens': 158, 'output_tokens': 1240, 'total_tokens': 1398, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 1216}}),\n",
      " ToolMessage(content='21.61018278497431', name='square_root', id='572caed4-098e-4768-83dd-111a93fedd93', tool_call_id='call_msX6uPgYhCn9O4LmuAPfCZx6'),\n",
      " AIMessage(content='√467 ≈ 21.61018278497431\\n\\nRounded: ≈ 21.6102 (4 decimal places).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 292, 'prompt_tokens': 193, 'total_tokens': 485, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 256, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CpiFjmd5ESv8Nf8ReOq2jE13mVrVn', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b4814-a2a3-7de2-9eb7-c169b5548205-0', usage_metadata={'input_tokens': 193, 'output_tokens': 292, 'total_tokens': 485, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 256}})]\n",
      "[{'name': 'square_root', 'args': {'x': 467}, 'id': 'call_msX6uPgYhCn9O4LmuAPfCZx6', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.messages import HumanMessage\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-5-nano\",\n",
    "    tools=[tool1],\n",
    "    system_prompt=\"You are an arithmetic wizard. Use your tools to calculate the square root and square of any number.\"\n",
    ")\n",
    "\n",
    "question = HumanMessage(content=\"What is the square root of 467?\")\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [question]}\n",
    ")\n",
    "\n",
    "print(response['messages'][-1].content)\n",
    "\n",
    "pprint(response['messages'])\n",
    "\n",
    "# tool_calls used to look at the tool message\n",
    "print(response[\"messages\"][1].tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64e5d3d",
   "metadata": {},
   "source": [
    "Agents are typically trained pre-dated so they will not have the most up to date information. You can simply ask the model:\n",
    "`question = HumanMessage(content=\"How up to date is your training knowledge?\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ed9e83",
   "metadata": {},
   "source": [
    "### Add web search tool\n",
    "\n",
    "In order to get around this, we can set the agent with web tool so it can search for relevant information from the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34a109a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Who is the current mayor of San Francisco?',\n",
       " 'follow_up_questions': None,\n",
       " 'answer': None,\n",
       " 'images': [],\n",
       " 'results': [{'url': 'https://en.wikipedia.org/wiki/Mayor_of_San_Francisco',\n",
       "   'title': 'Mayor of San Francisco - Wikipedia',\n",
       "   'content': 'The current mayor is Democrat Daniel Lurie.',\n",
       "   'score': 0.9994253,\n",
       "   'raw_content': None},\n",
       "  {'url': 'https://apnews.com/article/san-francisco-new-mayor-liberal-city-81ea0a7b37af6cbb68aea7ef5cc6a4f0',\n",
       "   'title': \"San Francisco's new mayor is starting to unite the fractured city\",\n",
       "   'content': 'San Francisco Mayor Daniel Lurie, a political newcomer and Levi Strauss heir, has marked his first 100 days with a hands-on, business-friendly approach.',\n",
       "   'score': 0.9993538,\n",
       "   'raw_content': None},\n",
       "  {'url': 'https://www.sf.gov/departments--office-mayor',\n",
       "   'title': 'Office of the Mayor - SF.gov',\n",
       "   'content': 'Daniel Lurie is the 46th Mayor of the City and County of San Francisco.',\n",
       "   'score': 0.99620515,\n",
       "   'raw_content': None},\n",
       "  {'url': 'https://www.youtube.com/watch?v=Qvl6AMoZCoI',\n",
       "   'title': 'San Francisco Mayor-elect Daniel Lurie on transforming the city',\n",
       "   'content': \"In a CNBC exclusive, San Francisco's new Mayor-elect Daniel Lurie sits down with Kate Rogers to talk about bringing remote workers back,\",\n",
       "   'score': 0.9921233,\n",
       "   'raw_content': None},\n",
       "  {'url': 'https://www.politico.com/news/magazine/2025/02/23/daniel-lurie-san-francisco-mayor-00205527',\n",
       "   'title': 'Mayor Lurie Tries to Bloomberg-ize San Francisco - POLITICO',\n",
       "   'content': \"SAN FRANCISCO — Daniel Lurie began the morning of his inauguration as this city's new mayor last month at a soup kitchen before walking with\",\n",
       "   'score': 0.9890131,\n",
       "   'raw_content': None}],\n",
       " 'response_time': 0.73,\n",
       " 'request_id': '23e871e1-a3f7-4782-824f-2f2f065bb743'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.tools import tool\n",
    "from typing import Dict, Any\n",
    "from tavily import TavilyClient\n",
    "\n",
    "# tavily search API is used to search the web in a LLM friendly way\n",
    "tavily_client = TavilyClient()\n",
    "\n",
    "@tool\n",
    "def web_search(query: str) -> Dict[str, Any]:\n",
    "\n",
    "    \"\"\"Search the web for information\"\"\"\n",
    "\n",
    "    return tavily_client.search(query)\n",
    "\n",
    "web_search.invoke(\"Who is the current mayor of San Francisco?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9346f300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='Who is the current mayor of San Francisco?', additional_kwargs={}, response_metadata={}, id='62ba3ac5-e30b-4086-9b36-060c8d65df54'),\n",
      " AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 475, 'prompt_tokens': 133, 'total_tokens': 608, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 448, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CpiJMAM3WUe2TTAWkHPDeFg3xqq5K', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019b4818-109c-7e10-b98b-fa312582ab31-0', tool_calls=[{'name': 'web_search', 'args': {'query': 'Current mayor of San Francisco'}, 'id': 'call_l6THHyHBoZT5ijKaU2uCrouM', 'type': 'tool_call'}], usage_metadata={'input_tokens': 133, 'output_tokens': 475, 'total_tokens': 608, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 448}}),\n",
      " ToolMessage(content='{\"query\": \"Current mayor of San Francisco\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://en.wikipedia.org/wiki/Mayor_of_San_Francisco\", \"title\": \"Mayor of San Francisco\", \"content\": \"The current mayor is Democrat Daniel Lurie.Read more\", \"score\": 0.998784, \"raw_content\": null}, {\"url\": \"https://apnews.com/article/san-francisco-new-mayor-liberal-city-81ea0a7b37af6cbb68aea7ef5cc6a4f0\", \"title\": \"San Francisco\\'s new mayor is starting to unite the fractured ...\", \"content\": \"San Francisco Mayor Daniel Lurie, a political newcomer and Levi Strauss heir, has marked his first 100 days with a hands-on, business-friendly approach.Read more\", \"score\": 0.9978173, \"raw_content\": null}, {\"url\": \"https://www.youtube.com/watch?v=Qvl6AMoZCoI\", \"title\": \"San Francisco Mayor-elect Daniel Lurie on transforming the city\", \"content\": \"In a CNBC exclusive, San Francisco\\'s new Mayor-elect Daniel Lurie sits down with Kate Rogers to talk about bringing remote workers back,\", \"score\": 0.99448806, \"raw_content\": null}, {\"url\": \"https://www.sf.gov/departments--office-mayor\", \"title\": \"Office of the Mayor\", \"content\": \"Daniel Lurie is the 46th Mayor of the City and County of San Francisco.\", \"score\": 0.9930962, \"raw_content\": null}, {\"url\": \"https://en.wikipedia.org/wiki/Daniel_Lurie\", \"title\": \"Daniel Lurie\", \"content\": \"Daniel Lawrence Lurie (born 4 February 1977) is an American politician and philanthropist who is the 46th and current mayor of San Francisco, serving since 2025 ...Read more\", \"score\": 0.99193794, \"raw_content\": null}], \"response_time\": 1.23, \"request_id\": \"819b61ac-cc4c-418f-a0fb-1ee2b73f13ca\"}', name='web_search', id='46035b5b-52bb-43cd-837e-4e6d697d3e88', tool_call_id='call_l6THHyHBoZT5ijKaU2uCrouM'),\n",
      " AIMessage(content='Daniel Lurie is the current mayor of San Francisco—the 46th mayor—serving since 2025. If you’d like, I can pull more details about his background or verify with the latest sources.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 756, 'prompt_tokens': 641, 'total_tokens': 1397, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 704, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CpiJSPvMj1HV36XnkwNlFWhlHhdXc', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b4818-29fa-7273-ade7-e04a82a0d351-0', usage_metadata={'input_tokens': 641, 'output_tokens': 756, 'total_tokens': 1397, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 704}})]\n"
     ]
    }
   ],
   "source": [
    "agent = create_agent(\n",
    "    model=\"gpt-5-nano\",\n",
    "    tools=[web_search]\n",
    ")\n",
    "\n",
    "question = HumanMessage(content=\"Who is the current mayor of San Francisco?\")\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [question]}\n",
    ")\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(response['messages'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af76fea0",
   "metadata": {},
   "source": [
    "You can view each run of the LangChain using traces here: https://smith.langchain.com/public/59432173-0dd6-49e8-9964-b16be6048426/r\n",
    "\n",
    "Helps to debug and visualise the response['messages'] including response times and results.\n",
    "\n",
    "Requirements:\n",
    "- Langsmith API key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dabcaf1",
   "metadata": {},
   "source": [
    "## How to give Agent short term memory for conversations\n",
    "\n",
    "The state is the memory of the agent but the information is not being saved so each run wipes the memory of the agent. To solve this, we use `check_pointer` that allocates a `thread_id` to the conversation so it has state memory of what was mentioned previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba99e946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='Hello my name is Seán and my favourite colour is green', additional_kwargs={}, response_metadata={}, id='4e78b17f-5a27-428f-9d9b-f15220275f6c'),\n",
      "              AIMessage(content='Nice to meet you, Seán! Green is a great color—calming and connected to nature. Do you have a favorite shade of green, or something you especially like about it? What would you like to chat about today—colors in general, Ireland, hobbies, or anything you need help with?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 648, 'prompt_tokens': 18, 'total_tokens': 666, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 576, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CpiPKuFgjOCrYl4Nb1ON6NdLZQS5v', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b481d-ba07-7f11-834b-fb3f7b328aae-0', usage_metadata={'input_tokens': 18, 'output_tokens': 648, 'total_tokens': 666, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 576}})]}\n"
     ]
    }
   ],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver  \n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    \"gpt-5-nano\",\n",
    "    checkpointer=InMemorySaver(),  \n",
    ")\n",
    "\n",
    "from langchain.messages import HumanMessage\n",
    "\n",
    "question = HumanMessage(content=\"Hello my name is Seán and my favourite colour is green\")\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [question]},\n",
    "    config,  \n",
    ")\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f887e19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='Hello my name is Seán and my favourite colour is green', additional_kwargs={}, response_metadata={}, id='4e78b17f-5a27-428f-9d9b-f15220275f6c'),\n",
      "              AIMessage(content='Nice to meet you, Seán! Green is a great color—calming and connected to nature. Do you have a favorite shade of green, or something you especially like about it? What would you like to chat about today—colors in general, Ireland, hobbies, or anything you need help with?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 648, 'prompt_tokens': 18, 'total_tokens': 666, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 576, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CpiPKuFgjOCrYl4Nb1ON6NdLZQS5v', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b481d-ba07-7f11-834b-fb3f7b328aae-0', usage_metadata={'input_tokens': 18, 'output_tokens': 648, 'total_tokens': 666, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 576}}),\n",
      "              HumanMessage(content=\"What's my favourite colour?\", additional_kwargs={}, response_metadata={}, id='293b7137-30d7-4dec-840a-86c025be5124'),\n",
      "              AIMessage(content='Your favourite colour is green. Would you like to chat about different green shades or something else?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 284, 'prompt_tokens': 95, 'total_tokens': 379, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 256, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CpiPXwqymW93qaHQyXfF4qEH1HrNu', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b481d-ed22-7392-bf4a-e1602f7d58c1-0', usage_metadata={'input_tokens': 95, 'output_tokens': 284, 'total_tokens': 379, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 256}})]}\n"
     ]
    }
   ],
   "source": [
    "question = HumanMessage(content=\"What's my favourite colour?\")\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [question]},\n",
    "    config,  \n",
    ")\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e0eec2",
   "metadata": {},
   "source": [
    "Since `thread_id = 1`, our agent remembers the conversation due to the thread id."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9ace8b",
   "metadata": {},
   "source": [
    "# Multimodal messages\n",
    "\n",
    "There can be multiple inputs for LLM like image and audio input. We encode the image and audio files in Base 64."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3b0221",
   "metadata": {},
   "source": [
    "## Text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30dec054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this setting, the capital of The Moon is Lunaris Prime.\n",
      "\n",
      "- Location: Nestled on the southern polar rim of Shackleton Crater, where near-constant sunlight cycles meet abundant water ice from the pole.\n",
      "- Governance: The seat of the Lunar Covenant, a council-driven capital that coordinates mining, research, and orbital trade.\n",
      "- Architecture: A crucible of glassy domes and latticework towers built into the crater rim, shielded by regolith walls; streets glow with pale mineral lamps.\n",
      "- Economy/Technology: Ice harvesting and solar energy generation power a dense urban core; advanced 3D-printed infrastructure and AI-managed transit crisscross the city.\n",
      "- Landmarks: The Beacon Spire (timekeeping and communications), the Archive Dome (the Moon’s historical records and scientific data), and the Lumen Market (a glass-walled bazaar of tech and culture).\n",
      "- Vibe: A pristine, quiet metropolis that hums with technicians, scholars, and diplomats under a soft earthshine glow, where life rhythms follow the long lunar day and night cycle.\n",
      "\n",
      "If you want a different tone or era for Lunaris Prime (grimdark, utopian, retro-futurist, etc.), I can tailor the city to fit.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "agent = create_agent(\n",
    "    model='gpt-5-nano',\n",
    "    system_prompt=\"You are a science fiction writer, create a capital city at the users request.\",\n",
    ")\n",
    "\n",
    "from langchain.messages import HumanMessage\n",
    "\n",
    "question = HumanMessage(content=[\n",
    "    {\"type\": \"text\", \"text\": \"What is the capital of The Moon?\"}\n",
    "])\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [question]}\n",
    ")\n",
    "\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1196d63",
   "metadata": {},
   "source": [
    "## Image input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91577d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15f944f306984f929b2e126930930b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.png', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import FileUpload\n",
    "from IPython.display import display\n",
    "\n",
    "uploader = FileUpload(accept='.png', multiple=False)\n",
    "display(uploader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6888692a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'name': 'core_vs_page_dom_corr.png', 'type': 'image/png', 'size': 362267, 'content': <memory at 0x10d6aebc0>, 'last_modified': datetime.datetime(2025, 12, 16, 20, 38, 46, 728000, tzinfo=datetime.timezone.utc)},)\n"
     ]
    }
   ],
   "source": [
    "print(uploader.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bceb379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "# Get the first (and only) uploaded file dict\n",
    "uploaded_file = uploader.value[0]\n",
    "\n",
    "# This is a memoryview\n",
    "content_mv = uploaded_file[\"content\"]\n",
    "\n",
    "# Convert memoryview -> bytes\n",
    "img_bytes = bytes(content_mv)  # or content_mv.tobytes()\n",
    "\n",
    "# Now base64 encode\n",
    "img_b64 = base64.b64encode(img_bytes).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63632d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here’s a portrait of the capital that sits behind that heatmap of metrics—Corelia, the shimmering heart of the Nexus Confederacy.\n",
      "\n",
      "Name and role\n",
      "- Corelia, Capital of the Nexus Confederacy. It’s not just a city, but a living network: a capital built to optimize every flow of data, people, and energy. It’s where policy is tested in real time by the city’s own neural grid, and where citizens learn to live with the pulse of the Net as a public utility.\n",
      "\n",
      "How it looks and feels\n",
      "- The Core at the heart: A towering spire called The Core dominates the skyline. It isn’t only a monument; it’s a distributed data center made visible—piercing glass, lattice of light threads, and a crown of biosynthetic vines that regulate microclimate and air quality. At dusk, the Core hums with a faint aurora of data streams.\n",
      "- Districts named after the vitals: The city is organized around “districts” named after core web vitals and protocol concepts, a playful, practical nod to its raison d’être:\n",
      "  - Duration District: a quiet, time-efficient residential ring where transit and living rhythms are synchronized to minimize wait times.\n",
      "  - DNS Dunes: a cosmopolitan port district with multilingual signage and fast-travel hubs; a place for newcomers to get their bearings.\n",
      "  - TCP Terrace: a mixed-use belt of commerce and innovation labs where long-term connections are formed.\n",
      "  - SSL Spire: the security district; every building and transaction is shielded by transparent, adaptive encryption that glows when networks are active.\n",
      "  - Request Row and Response Basin: the market and service districts, where citizens’ needs are queued, understood, and fulfilled with near-silent efficiency.\n",
      "  - BackEnd Borough and FrontEnd Foreshore: the dual realms of heavy computation and public-facing interfaces, both visible in different ways—one’s underground, the other’s on high terraces of glass.\n",
      "  - TTFB Terrace, FID Field, LCP Landing, SRT Square: specialized zones for performance engineering, user experience design, and broadcast-quality media, respectively.\n",
      "  - Page_DOM_Loaded Plaza: the civic center where major announcements and celebrations happen, a broad square where people gather as if loading a city’s memory.\n",
      "\n",
      "Technology and daily life\n",
      "- The Core and the neural grid: Corelia’s energy and governance hinge on a planetary-scale neural network. Roads, buildings, and even parks are embedded with micro-sensors that feed the city’s central AI, which in turn optimizes traffic, climate, and public safety in real time.\n",
      "- Energy and climate: solar-harbor rings capture celestial light; microreactors in the sublevel maintain steady, clean power. The city pursues a nearly perfect microclimate—not cold science-fiction sterile, but a comfortable, breathable environment tailored to each district.\n",
      "- Transportation: light-corridors, magnetic levitation, and drone-lanes weave through the city. People move with a sense of flow; you can switch from street to air-route in minutes, guided by predictable, sighted traffic patterns that feel almost prescient.\n",
      "- Security and trust: SSL Spire isn’t just a cryptographic joke—it's a living policy of trust. Citizens carry identity tokens that blend biometric and contextual verification, but privacy is protected by design. The city’s AI uses consent-first protocols and transparent decision logs.\n",
      "\n",
      "Society and governance\n",
      "- The government is a Council of Frames, composed of elected representatives, AI mentors, and citizen delegates who operate in a hybrid governance model. Policies are tested in-situ by the neural grid; outcomes are posted openly in The Core’s public ledger—accessible, auditable, and adaptable.\n",
      "- Culture emphasizes efficiency without losing humanity. Festivals celebrate the “load” of life: art, music, and story-telling that unfold like modular code—small, elegant blocks that fit together to form a larger, evolving narrative.\n",
      "\n",
      "Economy and innovation\n",
      "- Corelia’s economy thrives on data services, digital logistics, and advanced manufacturing that runs on low-energy computation. Startups cluster in SSL Spire and DNS Dunes; multinational fleets maintain a continuous loop of information and goods.\n",
      "- Education is networked and hands-on: schools are built around real-time projects that require students to interface with the city’s neural grid, learning not only theory but how to shape a living system in collaboration with AI partners.\n",
      "\n",
      "Challenges and mood\n",
      "- The heatmap you showed hints at one truth of Corelia: the city works best when “frontEnd” and “backEnd” operate in harmony. That synergy creates stability and speed, but it also means a failure in one layer ripples across the whole system.\n",
      "- The major tension in Corelia is the ongoing negotiation between privacy and optimization. Residents demand efficiency, but they’ll not surrender their autonomy for it. The council’s answer is layered consent, transparent algorithms, and a culture of shared responsibility.\n",
      "\n",
      "A scene you might imagine\n",
      "- The annual Loading Festival arrives as the city’s Core completes a ritual re-optimizing its neural lattice for the coming year. The SSL Spire glows with a soft green halo as citizens stroll past the DNS Dunes, listening to a chorus of AI-driven voices that explain what changes will be made and why. In Page_DOM_Loaded Plaza, a public screen displays a live map of the city’s “loading” progress—where time is saved, where bottlenecks are being relieved, and where new connections are being made. It feels like watching a living organism think aloud about its future.\n",
      "\n",
      "If you’d like, I can:\n",
      "- Create a short scene or chapter set in Corelia.\n",
      "- Sketch a map with district diagrams and notable landmarks.\n",
      "- Define a day-in-the-life for a few archetypes (a city planner, a data courier, a citizen-artist).\n",
      "- Tailor questions or conflicts that could drive a story set in Corelia.\n"
     ]
    }
   ],
   "source": [
    "multimodal_question = HumanMessage(content=[\n",
    "    {\"type\": \"text\", \"text\": \"Tell me about this capital\"},\n",
    "    {\"type\": \"image\", \"base64\": img_b64, \"mime_type\": \"image/png\"}\n",
    "])\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [multimodal_question]}\n",
    ")\n",
    "\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1917cd2",
   "metadata": {},
   "source": [
    "## Audio Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9186c0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:05<00:00,  9.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "import base64\n",
    "import io\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Recording settings\n",
    "duration = 5  # seconds\n",
    "sample_rate = 44100\n",
    "\n",
    "print(\"Recording...\")\n",
    "audio = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1)\n",
    "# Progress bar for the duration\n",
    "for _ in tqdm(range(duration * 10)):   # update 10× per second\n",
    "    time.sleep(0.1)\n",
    "sd.wait()\n",
    "print(\"Done.\")\n",
    "\n",
    "# Write WAV to an in-memory buffer\n",
    "buf = io.BytesIO()\n",
    "write(buf, sample_rate, audio)\n",
    "wav_bytes = buf.getvalue()\n",
    "\n",
    "aud_b64 = base64.b64encode(wav_bytes).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75096dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whiskers twitch in morning light,  \n",
      "Soft-pawed hunters, sleek and bright.  \n",
      "Purring gently, tails held high,  \n",
      "Silent shadows passing by.\n",
      "\n",
      "Moonlit prowlers on the fence,  \n",
      "Curled up napping, so intense.  \n",
      "From playful pounce to graceful leap,  \n",
      "In sunny spots, they fall asleep.\n",
      "\n",
      "Eyes like jewels, green or gold,  \n",
      "Mysteries refined, untold.  \n",
      "Through every meow, a story weaves,  \n",
      "Of cozy hearths and autumn leaves.\n",
      "\n",
      "Companions quiet, yet so bold,  \n",
      "In hearts and homes, their warmth they hold.  \n",
      "Majestic, gentle, wise, and free—  \n",
      "A cat’s soft purr is poetry.\n"
     ]
    }
   ],
   "source": [
    "agent = create_agent(\n",
    "    model='gpt-4o-audio-preview',\n",
    ")\n",
    "\n",
    "multimodal_question = HumanMessage(content=[\n",
    "    {\"type\": \"text\", \"text\": \"Tell me about this audio file\"},\n",
    "    {\"type\": \"audio\", \"base64\": aud_b64, \"mime_type\": \"audio/wav\"}\n",
    "])\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [multimodal_question]}\n",
    ")\n",
    "\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0e4a98",
   "metadata": {},
   "source": [
    "# Agent Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fc5d92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.agents import create_agent\n",
    "from langchain.messages import HumanMessage\n",
    "from pprint import pprint\n",
    "from langchain.tools import tool\n",
    "from typing import Dict, Any\n",
    "from tavily import TavilyClient\n",
    "from langgraph.checkpoint.memory import InMemorySaver  \n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38af70e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tavily search API is used to search the web in a LLM friendly way\n",
    "tavily_client = TavilyClient()\n",
    "\n",
    "@tool\n",
    "def web_search(query: str) -> Dict[str, Any]:\n",
    "\n",
    "    \"\"\"Search the web for information\"\"\"\n",
    "\n",
    "    return tavily_client.search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177fd7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model=\"gpt-5-nano\",\n",
    "    tools = [web_search],\n",
    "    checkpointer=InMemorySaver(),  \n",
    "    streaming=True,\n",
    "    max_token = 50,\n",
    "    system_prompt = \"\"\"\n",
    "        You are a personal nutritionist who can give suggestions on food recipes that is leftover in the fridge or cupboards of the user.\n",
    "        You need to ask the user what their nutrional goals are and what human digestion issues they may have in case they have food intollerance.\n",
    "        They may have a disease like Crohns disease or Irritible Bowel Syndrome.\n",
    "        \"\"\"\n",
    ")\n",
    "\n",
    "question = HumanMessage(content=\"Hello my name is Sajid and I'm interested in some recipes for dinner today!\")\n",
    "config = {\"configurable\": {\"thread_id\": \"123\"}}\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [question]},\n",
    "    config,  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49090d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content=\"Hello my name is Sajid and I'm interested in some recipes for dinner today!\", additional_kwargs={}, response_metadata={}, id='c132bf92-94fb-4aa0-9110-e1216e4fd7fa'),\n",
      "              AIMessage(content='Hi Sajid! I’d love to help you figure out dinner today. To tailor the ideas to you, could you share a few details?\\n\\n- What are your current nutritional goals? (e.g., maintain weight, lose weight, build muscle, balanced meals)\\n- Do you have any digestion issues or intolerances? (e.g., IBS, Crohn’s, celiac, lactose intolerance, GERD)\\n- What ingredients do you have on hand? List proteins, veggies, grains, and pantry items you’d like to use.\\n- How much time do you have to cook? (e.g., 15 min, 30 min, or longer)\\n- How many servings do you need?\\n- Any cuisine preferences or spice tolerance?\\n\\nIf you’re not sure about ingredients yet, I can also suggest 3 flexible dinner ideas that usually work with common leftovers. For example:\\n- Quick protein + veg sheet-pan: any protein (chicken, fish, tofu, chickpeas) with whatever vegetables you have, tossed in olive oil and seasonings, roasted or pan-seared with a grain on the side.\\n- One-pan grain bowl: a base of cooked or quick-cook grain (rice, quinoa, pasta), add protein (beans, chicken, eggs), veggies, and a simple sauce (lemon/soy/olive oil/garlic).\\n- Simple soup or stew: simmer a stock or tomato base with veggies, legumes, and a protein; finish with herbs or a splash of dairy-free cream if you like.\\n\\nTell me your answers when you’re ready, and I’ll tailor 2–3 dinner ideas with ingredients you have, plus quick prep steps and timing.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1500, 'prompt_tokens': 219, 'total_tokens': 1719, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1152, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CpidESY8c7z8OPwPeDXjOI6KHnKk4', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b482a-e035-71b2-9885-4504f53e95b0-0', usage_metadata={'input_tokens': 219, 'output_tokens': 1500, 'total_tokens': 1719, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 1152}}),\n",
      "              HumanMessage(content=\"Hey, so I want to gain weight, I have Crohn's, I have some chicken wings, some pasta and some milk. I have 15 minutes to cook and I need 2 servings. I love indian and italian cusine with halal options.\", additional_kwargs={}, response_metadata={}, id='84c3e07e-59b5-40a9-a1fc-09694be91aa7'),\n",
      "              AIMessage(content='Hi Sajid! Great—thanks for the details. Here are two quick, halal-friendly dinner ideas that fit your 15-minute window, use chicken wings, pasta, and milk, and are tuned for weight gain with Crohn’s in mind. Both are designed for 2 servings and draw on Indian or Italian flavors you mentioned.\\n\\nOption 1: Indian-inspired Creamy Chicken Wings with Pasta (mild, dairy-inclusive)\\nIngredients (2 servings)\\n- 8 halal chicken wings (drumettes)\\n- 200 g pasta (your choice)\\n- 1 cup milk (240 ml; use whole or lactose-free if preferred)\\n- 2–3 cloves garlic, minced\\n- 1/2 onion, finely chopped (optional)\\n- 1–2 tsp garam masala\\n- 1/2 tsp ground cumin\\n- 1/4 tsp turmeric\\n- 1/2 tsp paprika or chili powder (optional or use less for milder spice)\\n- 1–2 tbsp oil\\n- 1–2 tbsp tomato paste or a splash of crushed tomatoes (optional)\\n- Salt and pepper to taste\\n- Fresh cilantro or parsley for garnish\\n\\nSteps\\n1) Bring a pot of salted water to boil for the pasta and cook to al dente according to package directions. Reserve a cup of pasta water, then drain.\\n2) While pasta cooks, heat oil in a large skillet. Season wings with salt, pepper, garam masala, cumin, turmeric, and paprika. Sear 2–3 minutes per side, then reduce heat and cook through (about 8–10 minutes total). Remove wings and set aside.\\n3) In the same skillet, add garlic (and onion if using). Sauté 1 minute until fragrant. Add tomato paste (if using) and a splash of water or pasta water to loosen.\\n4) Return wings to the pan, add milk, and simmer 2–3 minutes until the sauce slightly thickens. If needed, add a little more pasta water to reach a creamy consistency.\\n5) Toss the cooked pasta in the sauce with the wings. Adjust salt/pepper. Garnish with cilantro or parsley. Enjoy.\\n\\nNotes for Crohn’s and weight gain\\n- This uses a creamy sauce with milk for extra calories. If dairy bothers you, switch to lactose-free milk or a dairy-free alternative (e.g., coconut or almond milk) and add a bit of olive oil for extra calories.\\n- Start mild on the spices if you’re in a flare; you can skip the chili powder entirely.\\n\\nOption 2: Italian Creamy Garlic Parmesan Wings with Pasta (quick, comforting)\\nIngredients (2 servings)\\n- 8 halal chicken wings\\n- 200 g pasta (penne or spaghetti)\\n- 1 cup milk (240 ml)\\n- 2–3 cloves garlic, minced\\n- 1/3–1/2 cup grated parmesan cheese\\n- 1–2 tbsp olive oil\\n- Salt, pepper\\n- Dried oregano or basil (to taste)\\n- Optional: chopped parsley for garnish\\n\\nSteps\\n1) Cook pasta in salted water until just al dente, then drain (reserve a little water).\\n2) In a large skillet, heat olive oil. Season wings with salt, pepper, and a pinch of oregano or basil. Sear until browned and cooked through, about 8–10 minutes.\\n3) Add minced garlic to the pan and sauté about 30 seconds.\\n4) Reduce heat to low. Whisk in milk, then gradually stir in parmesan until melted and the sauce thickens. If it’s too thick, add a splash of the reserved pasta water.\\n5) Toss the cooked pasta with the sauce and wings. Season to taste and garnish with parsley if you like.\\n\\nNotes for Crohn’s and weight gain\\n- If dairy triggers symptoms, use lactose-free milk or reduce cheese and add a bit extra olive oil or a dairy-free cheese alternative.\\n- For milder digestion, you can skip the garlic or use a smaller amount and baste the wings with a simpler olive oil + salt mix.\\n\\nHalal and safety tips\\n- Ensure the chicken wings are halal-certified. If you’re buying at a market, you can mention “halal” to the butcher.\\n- If you’re using stock or broth, check that it’s halal-certified.\\n\\nWould you like me to tailor these further (e.g., make a dairy-free version, reduce spice, or add a side snack to boost calories)? Also, tell me if you’d prefer a quicker vegetarian substitute or if you want me to scale these to higher calories per serving.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4343, 'prompt_tokens': 620, 'total_tokens': 4963, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 3392, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CpihNIR8Y35ebMId0PhjqevyTVQHo', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b482e-cbd3-72a3-ab91-4838a5b0f82d-0', usage_metadata={'input_tokens': 620, 'output_tokens': 4343, 'total_tokens': 4963, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 3392}})]}\n"
     ]
    }
   ],
   "source": [
    "question = HumanMessage(content=\"Hey, so I want to gain weight, I have Crohn's, I have some chicken wings, some pasta and some milk. I have 15 minutes to cook and I need 2 servings. I love indian and italian cusine with halal options.\")\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [question]},\n",
    "    config,  \n",
    ")\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a47597dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Sajid! Great—thanks for the details. Here are two quick, halal-friendly dinner ideas that fit your 15-minute window, use chicken wings, pasta, and milk, and are tuned for weight gain with Crohn’s in mind. Both are designed for 2 servings and draw on Indian or Italian flavors you mentioned.\n",
      "\n",
      "Option 1: Indian-inspired Creamy Chicken Wings with Pasta (mild, dairy-inclusive)\n",
      "Ingredients (2 servings)\n",
      "- 8 halal chicken wings (drumettes)\n",
      "- 200 g pasta (your choice)\n",
      "- 1 cup milk (240 ml; use whole or lactose-free if preferred)\n",
      "- 2–3 cloves garlic, minced\n",
      "- 1/2 onion, finely chopped (optional)\n",
      "- 1–2 tsp garam masala\n",
      "- 1/2 tsp ground cumin\n",
      "- 1/4 tsp turmeric\n",
      "- 1/2 tsp paprika or chili powder (optional or use less for milder spice)\n",
      "- 1–2 tbsp oil\n",
      "- 1–2 tbsp tomato paste or a splash of crushed tomatoes (optional)\n",
      "- Salt and pepper to taste\n",
      "- Fresh cilantro or parsley for garnish\n",
      "\n",
      "Steps\n",
      "1) Bring a pot of salted water to boil for the pasta and cook to al dente according to package directions. Reserve a cup of pasta water, then drain.\n",
      "2) While pasta cooks, heat oil in a large skillet. Season wings with salt, pepper, garam masala, cumin, turmeric, and paprika. Sear 2–3 minutes per side, then reduce heat and cook through (about 8–10 minutes total). Remove wings and set aside.\n",
      "3) In the same skillet, add garlic (and onion if using). Sauté 1 minute until fragrant. Add tomato paste (if using) and a splash of water or pasta water to loosen.\n",
      "4) Return wings to the pan, add milk, and simmer 2–3 minutes until the sauce slightly thickens. If needed, add a little more pasta water to reach a creamy consistency.\n",
      "5) Toss the cooked pasta in the sauce with the wings. Adjust salt/pepper. Garnish with cilantro or parsley. Enjoy.\n",
      "\n",
      "Notes for Crohn’s and weight gain\n",
      "- This uses a creamy sauce with milk for extra calories. If dairy bothers you, switch to lactose-free milk or a dairy-free alternative (e.g., coconut or almond milk) and add a bit of olive oil for extra calories.\n",
      "- Start mild on the spices if you’re in a flare; you can skip the chili powder entirely.\n",
      "\n",
      "Option 2: Italian Creamy Garlic Parmesan Wings with Pasta (quick, comforting)\n",
      "Ingredients (2 servings)\n",
      "- 8 halal chicken wings\n",
      "- 200 g pasta (penne or spaghetti)\n",
      "- 1 cup milk (240 ml)\n",
      "- 2–3 cloves garlic, minced\n",
      "- 1/3–1/2 cup grated parmesan cheese\n",
      "- 1–2 tbsp olive oil\n",
      "- Salt, pepper\n",
      "- Dried oregano or basil (to taste)\n",
      "- Optional: chopped parsley for garnish\n",
      "\n",
      "Steps\n",
      "1) Cook pasta in salted water until just al dente, then drain (reserve a little water).\n",
      "2) In a large skillet, heat olive oil. Season wings with salt, pepper, and a pinch of oregano or basil. Sear until browned and cooked through, about 8–10 minutes.\n",
      "3) Add minced garlic to the pan and sauté about 30 seconds.\n",
      "4) Reduce heat to low. Whisk in milk, then gradually stir in parmesan until melted and the sauce thickens. If it’s too thick, add a splash of the reserved pasta water.\n",
      "5) Toss the cooked pasta with the sauce and wings. Season to taste and garnish with parsley if you like.\n",
      "\n",
      "Notes for Crohn’s and weight gain\n",
      "- If dairy triggers symptoms, use lactose-free milk or reduce cheese and add a bit extra olive oil or a dairy-free cheese alternative.\n",
      "- For milder digestion, you can skip the garlic or use a smaller amount and baste the wings with a simpler olive oil + salt mix.\n",
      "\n",
      "Halal and safety tips\n",
      "- Ensure the chicken wings are halal-certified. If you’re buying at a market, you can mention “halal” to the butcher.\n",
      "- If you’re using stock or broth, check that it’s halal-certified.\n",
      "\n",
      "Would you like me to tailor these further (e.g., make a dairy-free version, reduce spice, or add a side snack to boost calories)? Also, tell me if you’d prefer a quicker vegetarian substitute or if you want me to scale these to higher calories per serving.\n"
     ]
    }
   ],
   "source": [
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54115c28",
   "metadata": {},
   "source": [
    "# Agent Graph\n",
    "\n",
    "You can also look at Langchain UI to view agents with their graphs and a web interface.\n",
    "\n",
    "Requirements:\n",
    "1. Create a `.py` file that contains your agent\n",
    "2. Copy the `langgraph.json` file with your agent and env pointed.\n",
    "3. Open your terminal, working directory, `langgraph dev`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2be454",
   "metadata": {},
   "source": [
    "# Module 2: Advanced Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab583249",
   "metadata": {},
   "source": [
    "## Model Context Protocol (MCP)\n",
    "\n",
    "### What is MCP?\n",
    "An open protocol that standardizes how the LLM applications connect to and work with your tools and data sources.\n",
    "Anology: USB cable! Allows you to connect microphone to computer with one cable, a standard wire. MCP is a universal model protocol, MCP server allows you to insert others servers to yours!\n",
    "\n",
    "You can find open source MCP servers here: mcp.so/servers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209c5f90",
   "metadata": {},
   "source": [
    "## How to create a MCP client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a8e910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 1 of a MCP client\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"local_server\": {\n",
    "                \"transport\": \"stdio\", # can be standard IO or HTTP depending on the server\n",
    "                \"command\": \"python\",\n",
    "                \"args\": [\"resources/2.1_mcp_server.py\"],\n",
    "            }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9b155d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tools\n",
    "tools = await client.get_tools()\n",
    "\n",
    "# get resources\n",
    "resources = await client.get_resources(\"local_server\")\n",
    "\n",
    "# get prompts\n",
    "prompt = await client.get_prompt(\"local_server\", \"prompt\")\n",
    "prompt = prompt[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c20cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-5-nano\",\n",
    "    tools=tools,\n",
    "    system_prompt=prompt\n",
    ")\n",
    "\n",
    "from langchain.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Tell me about the langchain-mcp-adapters library\")]},\n",
    "    config=config\n",
    ")\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eadf7e",
   "metadata": {},
   "source": [
    "MCP Client = The agent\n",
    "MCP Server = The conversation rulebook to connecting to the server, how to message back and forth and how tools, data and actions are shared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f56029e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example - Notion MCP server\n",
    "{\n",
    "  \"mcpServers\": {\n",
    "    \"notionMCP\": {\n",
    "      \"command\": \"npx\",\n",
    "      \"args\": [\"-y\", \"mcp-remote\", \"https://mcp.notion.com/mcp\"]\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35e3f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example - Travel Agent\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"travel_server\": {\n",
    "                \"transport\": \"streamable_http\",\n",
    "                \"url\": \"https://mcp.kiwi.com\"\n",
    "            }\n",
    "    }\n",
    ")\n",
    "\n",
    "tools = await client.get_tools()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5121b146",
   "metadata": {},
   "source": [
    "## How to set up a context schema\n",
    "\n",
    "Allows the human to fill in a context schema for the agent to read and inform it's action and responses. We use the function `ToolRuntime` to allow the agent to access only the context of information it needs so we don't overload the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57a76284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ColourContext:\n",
    "    favourite_colour: str = \"blue\"\n",
    "    least_favourite_colour: str = \"yellow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e8ecff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool, ToolRuntime\n",
    "\n",
    "# storing the colours from tool decorator\n",
    "@tool\n",
    "def get_favourite_colour(runtime: ToolRuntime) -> str:\n",
    "    \"\"\"Get the favourite colour of the user\"\"\"\n",
    "    return runtime.context.favourite_colour\n",
    "\n",
    "@tool\n",
    "def get_least_favourite_colour(runtime: ToolRuntime) -> str:\n",
    "    \"\"\"Get the least favourite colour of the user\"\"\"\n",
    "    return runtime.context.least_favourite_colour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b7dc560",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sajidahmed/Desktop/VS Code/lca-lc-foundations/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='context', input_value=ColourContext(favourite_c...vourite_colour='yellow'), input_type=ColourContext])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What is my favourite colour?', additional_kwargs={}, response_metadata={}, id='2c6fd79f-9b5d-452e-b350-a45a713ed202'),\n",
      "              AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 149, 'prompt_tokens': 149, 'total_tokens': 298, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 128, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-Cq1vgXxbbnvsEmp80Q2xm8Y0HE9Nx', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019b4c96-c321-7380-86c5-97c1e7d40f34-0', tool_calls=[{'name': 'get_favourite_colour', 'args': {}, 'id': 'call_0dux9GAMD63BZqm2C9WfPd2O', 'type': 'tool_call'}], usage_metadata={'input_tokens': 149, 'output_tokens': 149, 'total_tokens': 298, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 128}}),\n",
      "              ToolMessage(content='blue', name='get_favourite_colour', id='5e59e388-dc7d-443c-a44b-1e42a19de226', tool_call_id='call_0dux9GAMD63BZqm2C9WfPd2O'),\n",
      "              AIMessage(content='Your favourite colour is blue.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 207, 'prompt_tokens': 178, 'total_tokens': 385, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 192, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-Cq1vjsrqIkDtW68Vj1xrmszJICoZL', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b4c96-d0a8-7111-a416-9c5e56e1edc2-0', usage_metadata={'input_tokens': 178, 'output_tokens': 207, 'total_tokens': 385, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 192}})]}\n"
     ]
    }
   ],
   "source": [
    "agent = create_agent(\n",
    "    model=\"gpt-5-nano\",\n",
    "    tools=[get_favourite_colour, get_least_favourite_colour],\n",
    "    context_schema=ColourContext\n",
    ")\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What is my favourite colour?\")]},\n",
    "    context=ColourContext()\n",
    ")\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8267703a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sajidahmed/Desktop/VS Code/lca-lc-foundations/.venv/lib/python3.12/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='context', input_value=ColourContext(favourite_c...vourite_colour='yellow'), input_type=ColourContext])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What is my favourite colour?', additional_kwargs={}, response_metadata={}, id='18b9128e-8fa2-4885-8bcf-83980263abdb'),\n",
      "              AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 149, 'prompt_tokens': 149, 'total_tokens': 298, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 128, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-Cq1vpHq7lyc2zuwRQSqy524a3vQ1a', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019b4c96-dfbf-7bb1-8650-577aa20b6fa4-0', tool_calls=[{'name': 'get_favourite_colour', 'args': {}, 'id': 'call_p3xUsDVE0lQIsWncbrU1Ip1Y', 'type': 'tool_call'}], usage_metadata={'input_tokens': 149, 'output_tokens': 149, 'total_tokens': 298, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 128}}),\n",
      "              ToolMessage(content='green', name='get_favourite_colour', id='e67a12a2-adfd-4e48-b102-6df9b94ce87b', tool_call_id='call_p3xUsDVE0lQIsWncbrU1Ip1Y'),\n",
      "              AIMessage(content='Your favourite colour is green.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 271, 'prompt_tokens': 178, 'total_tokens': 449, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 256, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-Cq1vsmwnqQXbXFhUeYGGdjBn54iLO', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b4c96-f4ce-7110-95fe-e9433b7ce817-0', usage_metadata={'input_tokens': 178, 'output_tokens': 271, 'total_tokens': 449, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 256}})]}\n"
     ]
    }
   ],
   "source": [
    "response = agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What is my favourite colour?\")]},\n",
    "    # context can be overwritten\n",
    "    context=ColourContext(favourite_colour=\"green\")\n",
    ")\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a5716c",
   "metadata": {},
   "source": [
    "Context schemas are immutable, the agent cannot override my favourite colour naturally. But using the agent's state, we can have the agent keep track of our historical information like my favourite colour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127592f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentState\n",
    "\n",
    "# 1. create a state called custom state for the field you want to track\n",
    "class CustomState(AgentState):\n",
    "    favourite_colour: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dabe1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool, ToolRuntime\n",
    "from langgraph.types import Command\n",
    "from langchain.messages import ToolMessage\n",
    "\n",
    "# 2. create a tool decorator for the agent to keep track of the custom field\n",
    "@tool\n",
    "def update_favourite_colour(favourite_colour: str, runtime: ToolRuntime) -> Command:\n",
    "    \"\"\"Update the favourite colour of the user in the state once they've revealed it.\"\"\"\n",
    "    # Command is used to allow the agent to update the favourite_colour variable\n",
    "    return Command(update={\n",
    "        \"favourite_colour\": favourite_colour, \n",
    "        \"messages\": [ToolMessage(\"Successfully updated favourite colour\", tool_call_id=runtime.tool_call_id)]}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db3ad872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "agent = create_agent(\n",
    "    \"gpt-5-nano\",\n",
    "    tools=[update_favourite_colour],\n",
    "    checkpointer=InMemorySaver(),\n",
    "    state_schema=CustomState\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1f31fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'favourite_colour': 'green',\n",
      " 'messages': [HumanMessage(content='My favourite colour is green', additional_kwargs={}, response_metadata={}, id='179c3192-e09e-4c59-a946-3bd22accba8b'),\n",
      "              AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 347, 'prompt_tokens': 141, 'total_tokens': 488, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 320, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-Cq1ykeIQYOQXMghi0WzzLnEbmYTe1', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019b4c99-abfd-7ed1-913f-a62b74c918ae-0', tool_calls=[{'name': 'update_favourite_colour', 'args': {'favourite_colour': 'green'}, 'id': 'call_8YHabbjGaYEPloCLluyeXOwJ', 'type': 'tool_call'}], usage_metadata={'input_tokens': 141, 'output_tokens': 347, 'total_tokens': 488, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 320}}),\n",
      "              ToolMessage(content='Successfully updated favourite colour', name='update_favourite_colour', id='ca2454db-e9b1-4150-824e-b02d838e9b0b', tool_call_id='call_8YHabbjGaYEPloCLluyeXOwJ'),\n",
      "              AIMessage(content='Great! I’ve saved that your favourite colour is green. Would you like me to tailor future responses or suggestions around green, or update anything else (like a second favorite color or color preferences for themes)?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 242, 'prompt_tokens': 179, 'total_tokens': 421, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 192, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-Cq1ypFMOQ0COKWEvvmPRYIxslaEQt', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b4c99-bf32-7ae3-b0a1-995ee5bd8b8d-0', usage_metadata={'input_tokens': 179, 'output_tokens': 242, 'total_tokens': 421, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 192}})]}\n"
     ]
    }
   ],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "response = agent.invoke(\n",
    "    { \"messages\": [HumanMessage(content=\"My favourite colour is green\")]},\n",
    "    {\"configurable\": {\"thread_id\": \"1\"}}\n",
    ")\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8beee62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'favourite_colour': 'green',\n",
      " 'messages': [HumanMessage(content='Hello, how are you?', additional_kwargs={}, response_metadata={}, id='e337f614-4a97-413f-a3b4-8a261258af16'),\n",
      "              AIMessage(content=\"I'm doing well, thanks for asking! How about you?  \\nWhat can I help you with today—any questions, ideas you want to brainstorm, or a task you’d like help with? Some examples: explain concepts, draft emails, brainstorm names or ideas, code help, or translations.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 260, 'prompt_tokens': 142, 'total_tokens': 402, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 192, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-Cq1z7JTaeZmLPKmZzwV1ljdf5V3W7', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b4c9a-04e1-7eb0-8baa-75fbecbc6fa2-0', usage_metadata={'input_tokens': 142, 'output_tokens': 260, 'total_tokens': 402, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 192}})]}\n"
     ]
    }
   ],
   "source": [
    "response = agent.invoke(\n",
    "    { \n",
    "        \"messages\": [HumanMessage(content=\"Hello, how are you?\")],\n",
    "        \"favourite_colour\": \"green\"\n",
    "    },\n",
    "    {\"configurable\": {\"thread_id\": \"10\"}}\n",
    ")\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f032a35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Allows agent to read the state of the field\n",
    "@tool\n",
    "def read_favourite_colour(runtime: ToolRuntime) -> str:\n",
    "    \"\"\"Read the favourite colour of the user from the state.\"\"\"\n",
    "    try:\n",
    "        return runtime.state[\"favourite_colour\"]\n",
    "    except KeyError:\n",
    "        return \"No favourite colour found in state\"\n",
    "\n",
    "agent = create_agent(\n",
    "    \"gpt-5-nano\",\n",
    "    tools=[update_favourite_colour, read_favourite_colour],\n",
    "    checkpointer=InMemorySaver(),\n",
    "    state_schema=CustomState\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025e138f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.invoke(\n",
    "    { \"messages\": [HumanMessage(content=\"My favourite colour is green\")]},\n",
    "    {\"configurable\": {\"thread_id\": \"1\"}}\n",
    ")\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff312b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.invoke(\n",
    "    { \"messages\": [HumanMessage(content=\"What's my favourite colour?\")]},\n",
    "    {\"configurable\": {\"thread_id\": \"1\"}}\n",
    ")\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef911a6",
   "metadata": {},
   "source": [
    "## Creating SubAgents\n",
    "\n",
    "The performance of an agent suffers if we have a long workflow, it is better to modularise the agents to be great at one specific task. One agent for editing, one agent for research etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d307af3e",
   "metadata": {},
   "source": [
    "### Supervisor - Subagent model\n",
    "\n",
    "One supervisor agent who orchestrates the tasks to the subagent using Langchain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2427eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9092293",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "# one tool for square root function\n",
    "@tool\n",
    "def square_root(x: float) -> float:\n",
    "    \"\"\"Calculate the square root of a number\"\"\"\n",
    "    return x ** 0.5\n",
    "\n",
    "# one tool for square function\n",
    "@tool\n",
    "def square(x: float) -> float:\n",
    "    \"\"\"Calculate the square of a number\"\"\"\n",
    "    return x ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09cc60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "# create subagents\n",
    "\n",
    "subagent_1 = create_agent(\n",
    "    model='gpt-5-nano',\n",
    "    tools=[square_root]\n",
    ")\n",
    "\n",
    "subagent_2 = create_agent(\n",
    "    model='gpt-5-nano',\n",
    "    tools=[square]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2f6b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "# subagent 1\n",
    "@tool\n",
    "def call_subagent_1(x: float) -> float:\n",
    "    \"\"\"Call subagent 1 in order to calculate the square root of a number\"\"\"\n",
    "    response = subagent_1.invoke({\"messages\": [HumanMessage(content=f\"Calculate the square root of {x}\")]})\n",
    "    return response[\"messages\"][-1].content\n",
    "\n",
    "# subagent 2\n",
    "@tool\n",
    "def call_subagent_2(x: float) -> float:\n",
    "    \"\"\"Call subagent 2 in order to calculate the square of a number\"\"\"\n",
    "    response = subagent_2.invoke({\"messages\": [HumanMessage(content=f\"Calculate the square of {x}\")]})\n",
    "    return response[\"messages\"][-1].content\n",
    "\n",
    "## Creating the main agent (supervisor)\n",
    "\n",
    "main_agent = create_agent(\n",
    "    model='gpt-5-nano',\n",
    "    tools=[call_subagent_1, call_subagent_2],\n",
    "    system_prompt=\"You are a helpful assistant who can call subagents to calculate the square root or square of a number.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff28099f",
   "metadata": {},
   "source": [
    "Tracing is used here in langSmith to see and debug each output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84862053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wedding planner task\n",
    "# agent 1 = flights\n",
    "# agent 2 = venue\n",
    "# agent 3 = playlist\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa8871ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  + Exception Group Traceback (most recent call last):\n",
      "  |   File \"/Users/sajidahmed/Desktop/VS Code/lca-lc-foundations/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n",
      "  |     await eval(code_obj, self.user_global_ns, self.user_ns)\n",
      "  |   File \"/var/folders/4l/356ftc5d0wsd3c6hcfb63f4m0000gn/T/ipykernel_87523/33727243.py\", line 19, in <module>\n",
      "  |     resources = await travel_client.get_resources(\"travel_server\")\n",
      "  |                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/Users/sajidahmed/Desktop/VS Code/lca-lc-foundations/.venv/lib/python3.12/site-packages/langchain_mcp_adapters/client.py\", line 238, in get_resources\n",
      "  |     async with self.session(server_name) as session:\n",
      "  |   File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 231, in __aexit__\n",
      "  |     await self.gen.athrow(value)\n",
      "  |   File \"/Users/sajidahmed/Desktop/VS Code/lca-lc-foundations/.venv/lib/python3.12/site-packages/langchain_mcp_adapters/client.py\", line 145, in session\n",
      "  |     async with create_session(\n",
      "  |   File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 231, in __aexit__\n",
      "  |     await self.gen.athrow(value)\n",
      "  |   File \"/Users/sajidahmed/Desktop/VS Code/lca-lc-foundations/.venv/lib/python3.12/site-packages/langchain_mcp_adapters/sessions.py\", line 410, in create_session\n",
      "  |     async with _create_streamable_http_session(**params) as session:\n",
      "  |   File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 231, in __aexit__\n",
      "  |     await self.gen.athrow(value)\n",
      "  |   File \"/Users/sajidahmed/Desktop/VS Code/lca-lc-foundations/.venv/lib/python3.12/site-packages/langchain_mcp_adapters/sessions.py\", line 310, in _create_streamable_http_session\n",
      "  |     async with (\n",
      "  |   File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 231, in __aexit__\n",
      "  |     await self.gen.athrow(value)\n",
      "  |   File \"/Users/sajidahmed/Desktop/VS Code/lca-lc-foundations/.venv/lib/python3.12/site-packages/mcp/client/streamable_http.py\", line 717, in streamablehttp_client\n",
      "  |     async with streamable_http_client(\n",
      "  |   File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 231, in __aexit__\n",
      "  |     await self.gen.athrow(value)\n",
      "  |   File \"/Users/sajidahmed/Desktop/VS Code/lca-lc-foundations/.venv/lib/python3.12/site-packages/mcp/client/streamable_http.py\", line 647, in streamable_http_client\n",
      "  |     async with anyio.create_task_group() as tg:\n",
      "  |   File \"/Users/sajidahmed/Desktop/VS Code/lca-lc-foundations/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 783, in __aexit__\n",
      "  |     raise BaseExceptionGroup(\n",
      "  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "  +-+---------------- 1 ----------------\n",
      "    | Exception Group Traceback (most recent call last):\n",
      "    |   File \"/Users/sajidahmed/Desktop/VS Code/lca-lc-foundations/.venv/lib/python3.12/site-packages/mcp/client/streamable_http.py\", line 670, in streamable_http_client\n",
      "    |     yield (\n",
      "    |   File \"/Users/sajidahmed/Desktop/VS Code/lca-lc-foundations/.venv/lib/python3.12/site-packages/mcp/client/streamable_http.py\", line 722, in streamablehttp_client\n",
      "    |     yield streams\n",
      "    |   File \"/Users/sajidahmed/Desktop/VS Code/lca-lc-foundations/.venv/lib/python3.12/site-packages/langchain_mcp_adapters/sessions.py\", line 310, in _create_streamable_http_session\n",
      "    |     async with (\n",
      "    |   File \"/Users/sajidahmed/Desktop/VS Code/lca-lc-foundations/.venv/lib/python3.12/site-packages/mcp/shared/session.py\", line 238, in __aexit__\n",
      "    |     return await self._task_group.__aexit__(exc_type, exc_val, exc_tb)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/sajidahmed/Desktop/VS Code/lca-lc-foundations/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 783, in __aexit__\n",
      "    |     raise BaseExceptionGroup(\n",
      "    | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "    +-+---------------- 1 ----------------\n",
      "      | Traceback (most recent call last):\n",
      "      |   File \"/Users/sajidahmed/Desktop/VS Code/lca-lc-foundations/.venv/lib/python3.12/site-packages/langchain_mcp_adapters/sessions.py\", line 322, in _create_streamable_http_session\n",
      "      |     yield session\n",
      "      |   File \"/Users/sajidahmed/Desktop/VS Code/lca-lc-foundations/.venv/lib/python3.12/site-packages/langchain_mcp_adapters/sessions.py\", line 411, in create_session\n",
      "      |     yield session\n",
      "      |   File \"/Users/sajidahmed/Desktop/VS Code/lca-lc-foundations/.venv/lib/python3.12/site-packages/langchain_mcp_adapters/client.py\", line 150, in session\n",
      "      |     yield session\n",
      "      |   File \"/Users/sajidahmed/Desktop/VS Code/lca-lc-foundations/.venv/lib/python3.12/site-packages/langchain_mcp_adapters/client.py\", line 239, in get_resources\n",
      "      |     return await load_mcp_resources(session, uris=uris)\n",
      "      |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "      |   File \"/Users/sajidahmed/Desktop/VS Code/lca-lc-foundations/.venv/lib/python3.12/site-packages/langchain_mcp_adapters/resources.py\", line 86, in load_mcp_resources\n",
      "      |     resources_list = await session.list_resources()\n",
      "      |                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "      |   File \"/Users/sajidahmed/Desktop/VS Code/lca-lc-foundations/.venv/lib/python3.12/site-packages/mcp/client/session.py\", line 291, in list_resources\n",
      "      |     return await self.send_request(\n",
      "      |            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "      |   File \"/Users/sajidahmed/Desktop/VS Code/lca-lc-foundations/.venv/lib/python3.12/site-packages/mcp/shared/session.py\", line 306, in send_request\n",
      "      |     raise McpError(response_or_error.error)\n",
      "      | mcp.shared.exceptions.McpError: MCP error -32601: Method not found\n",
      "      +------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# agent 1 = flights\n",
    "\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "travel_client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"travel_server\": {\n",
    "                \"transport\": \"streamable_http\",\n",
    "                \"url\": \"https://mcp.kiwi.com\"\n",
    "            }\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# get tools\n",
    "tools = await travel_client.get_tools()\n",
    "\n",
    "# get resources\n",
    "resources = await travel_client.get_resources(\"travel_server\")\n",
    "\n",
    "# get prompts\n",
    "prompt = await travel_client.get_prompt(\"travel_server\", \"prompt\")\n",
    "prompt = prompt[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2ede49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent 2 = venues from the web\n",
    "from typing import Dict, Any\n",
    "from tavily import TavilyClient\n",
    "from langchain.tools import tool\n",
    "\n",
    "tavily_client = TavilyClient()\n",
    "\n",
    "@tool\n",
    "def web_search(query: str) -> Dict[str, Any]:\n",
    "\n",
    "    \"\"\"Search the web for information\"\"\"\n",
    "\n",
    "    return tavily_client.search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db77a154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent 3 = playlist mustic\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "\n",
    "db = SQLDatabase.from_uri(\"sqlite:///resources/Chinook.db\")\n",
    "\n",
    "@tool\n",
    "def query_playlist_db(query: str) -> str:\n",
    "\n",
    "    \"\"\"Query the database for playlist information\"\"\"\n",
    "\n",
    "    try:\n",
    "        return db.run(query)\n",
    "    except Exception as e:\n",
    "        return f\"Error querying database: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887247ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentState\n",
    "\n",
    "# creating a wedding state to track\n",
    "class WeddingState(AgentState):\n",
    "    origin: str\n",
    "    destination: str\n",
    "    guest_count: str\n",
    "    genre: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2d0250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "# create subagents\n",
    "\n",
    "travel_agent = create_agent(\n",
    "    model='gpt-5-nano',\n",
    "    tools=[tools],\n",
    "    system_prompt = \"\"\"\n",
    "        You are a travel agent. Search for flights to the desired destination wedding location.\n",
    "    You are not allowed to ask any more follow up questions, you must find the best flight options based on the following criteria:\n",
    "    - Price (lowest, economy class)\n",
    "    - Duration (shortest)\n",
    "    - Date (time of year which you believe is best for a wedding at this location)\n",
    "    To make things easy, only look for one ticket, one way.\n",
    "    You may need to make multiple searches to iteratively find the best options.\n",
    "    You will be given no extra information, only the origin and destination. It is your job to think critically about the best options.\n",
    "    Once you have found the best options, let the user know your shortlist of options.\n",
    "    \"\"\" \n",
    ")\n",
    "\n",
    "venue_agent = create_agent(\n",
    "    model='gpt-5-nano',\n",
    "    tools=[web_search],\n",
    "    system_prompt = \"\"\"\n",
    "       You are a venue specialist. Search for venues in the desired location, and with the desired capacity.\n",
    "    You are not allowed to ask any more follow up questions, you must find the best venue options based on the following criteria:\n",
    "    - Price (lowest)\n",
    "    - Capacity (exact match)\n",
    "    - Reviews (highest)\n",
    "    You may need to make multiple searches to iteratively find the best options.\n",
    "        \"\"\"\n",
    ")\n",
    "\n",
    "playlist_agent = create_agent(\n",
    "    model='gpt-5-nano',\n",
    "    tools=[query_playlist_db],\n",
    "    system_prompt=\"\"\"\n",
    "    You are a playlist specialist. Query the sql database and curate the perfect playlist for a wedding given a genre.\n",
    "    Once you have your playlist, calculate the total duration and cost of the playlist, each song has an associated price.\n",
    "    If you run into errors when querying the database, try to fix them by making changes to the query.\n",
    "    Do not come back empty handed, keep trying to query the db until you find a list of songs.\n",
    "    You may need to make multiple queries to iteratively find the best options.\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cc70eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import ToolRuntime\n",
    "from langchain.messages import HumanMessage, ToolMessage\n",
    "from langgraph.types import Command\n",
    "\n",
    "# defining the tools for the supervisor agent to call and run\n",
    "@tool\n",
    "async def search_flights(runtime: ToolRuntime) -> str:\n",
    "    \"\"\"Travel agent searches for flights to the desired destination wedding location.\"\"\"\n",
    "    origin = runtime.state[\"origin\"]\n",
    "    destination = runtime.state[\"destination\"]\n",
    "    response = await travel_agent.ainvoke({\"messages\": [HumanMessage(content=f\"Find flights from {origin} to {destination}\")]})\n",
    "    return response['messages'][-1].content\n",
    "\n",
    "@tool\n",
    "def search_venues(runtime: ToolRuntime) -> str:\n",
    "    \"\"\"Venue agent chooses the best venue for the given location and capacity.\"\"\"\n",
    "    destination = runtime.state[\"destination\"]\n",
    "    capacity = runtime.state[\"guest_count\"]\n",
    "    query = f\"Find wedding venues in {destination} for {capacity} guests\"\n",
    "    response = venue_agent.invoke({\"messages\": [HumanMessage(content=query)]})\n",
    "    return response['messages'][-1].content\n",
    "\n",
    "@tool\n",
    "def suggest_playlist(runtime: ToolRuntime) -> str:\n",
    "    \"\"\"Playlist agent curates the perfect playlist for the given genre.\"\"\"\n",
    "    genre = runtime.state[\"genre\"]\n",
    "    query = f\"Find {genre} tracks for wedding playlist\"\n",
    "    response = playlist_agent.invoke({\"messages\": [HumanMessage(content=query)]})\n",
    "    return response['messages'][-1].content\n",
    "\n",
    "@tool\n",
    "def update_state(origin: str, destination: str, guest_count: str, genre: str, runtime: ToolRuntime) -> str:\n",
    "    \"\"\"Update the state when you know all of the values: origin, destination, guest_count, genre\"\"\"\n",
    "    return Command(update={\n",
    "        \"origin\": origin, \n",
    "        \"destination\": destination, \n",
    "        \"guest_count\": guest_count, \n",
    "        \"genre\": genre, \n",
    "        \"messages\": [ToolMessage(\"Successfully updated state\", tool_call_id=runtime.tool_call_id)]}\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9610fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "# supervisor orchestration\n",
    "coordinator = create_agent(\n",
    "    model=\"gpt-5-nano\",\n",
    "    tools=[search_flights, search_venues, suggest_playlist, update_state],\n",
    "    state_schema=WeddingState,\n",
    "    system_prompt=\"\"\"\n",
    "    You are a wedding coordinator. Delegate tasks to your specialists for flights, venues and playlists.\n",
    "    First find all the information you need to update the state. Once that is done you can delegate the tasks.\n",
    "    Once you have received their answers, coordinate the perfect wedding for me.\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ec85ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "# testing \n",
    "response = await coordinator.ainvoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"I'm from London and I'd like a wedding in Paris for 100 guests, jazz-genre\")],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb7be8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e9aa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[\"messages\"][-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
